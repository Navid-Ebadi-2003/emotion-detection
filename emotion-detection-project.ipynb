{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a)\n",
    "1. Linear Kernel: The Linear Kernel is the simplest form of kernel functions in SVM. It is mainly used when the data is linearly separable. It calculates the dot product of two observations. The formula for the Linear Kernel is:\n",
    "\n",
    "    K(x,y)=xTy\n",
    "\n",
    "2. Polynomial Kernel: The Polynomial Kernel is a more generalized form of the linear kernel. It can distinguish curved or nonlinear input space. The Polynomial Kernel can be represented as:\n",
    "\n",
    "    K(x,y)=(γxTy+r)d\n",
    "\n",
    "    Here, γ is the slope, r is the intercept, and d is the degree of the polynomial.\n",
    "3. Radial Basis Function (RBF) Kernel: The RBF kernel is a popular kernel function commonly used in SVM classification. It can map an input space in infinite dimensional space. It is used when there is no prior knowledge about the data. The RBF Kernel is represented as:\n",
    "\n",
    "    K(x,y)=exp(−γ∣∣x−y∣∣2)\n",
    "\n",
    "    Here, γ is a parameter that sets the ‘spread’ of the function.\n",
    "\n",
    "4. Sigmoid Kernel: The Sigmoid Kernel uses the sigmoid function, commonly used in neural networks. The Sigmoid Kernel comes with the advantage of being applicable in neural networks. The Sigmoid Kernel is represented as:\n",
    "\n",
    "    K(x,y)=tanh(γxTy+r)\n",
    "\n",
    "    Here, γ is the slope, r is the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b)\n",
    "\n",
    "1. XGBoost: XGBoost, short for eXtreme Gradient Boosting, is a popular gradient boosting framework that is known for its speed and performance. It supports both linear and tree learning algorithms and provides a variety of regularization parameters to prevent overfitting. However, XGBoost does not handle categorical features directly. Instead, one-hot encoding or similar techniques must be applied to convert categorical data into numerical data before using XGBoost.\n",
    "\n",
    "2. LightGBM: LightGBM, short for Light Gradient Boosting Machine, is a gradient boosting framework developed by Microsoft. It is known for its speed and efficiency, especially on large datasets. LightGBM uses a novel technique of Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) to handle large-scale data with high efficiency. However, like XGBoost, LightGBM also does not natively handle categorical features and requires them to be converted into numerical data.\n",
    "\n",
    "3. CatBoost: CatBoost, short for Categorical Boosting, is a gradient boosting framework developed by Yandex. It is specifically designed to handle categorical features directly, using a novel algorithm called “ordered boosting”, a permutation-driven alternative to the classic algorithm. This makes CatBoost particularly effective on datasets with many categorical features. In terms of performance, CatBoost is often found to have the best generalization accuracy and AUC among the three, although the differences are small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c)\n",
    "\n",
    "1. k-fold Cross-Validation: In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation.\n",
    "\n",
    "2. Stratified k-fold Cross-Validation: Stratified k-fold cross-validation is a variation of k-fold cross-validation that provides more reliable and robust estimates especially when the data is imbalanced. In stratified k-fold cross-validation, the folds are selected so that the mean response value is approximately equal in all the folds. In the case of a dichotomous classification, this means that each fold contains roughly the same proportions of the two types of class labels.\n",
    "\n",
    "The main difference between the two methods is how they split the data. Regular k-fold cross-validation splits the data randomly, while stratified k-fold cross-validation splits the data in a way that preserves the proportion of samples for each class.\n",
    "\n",
    "Stratified k-fold cross-validation is used when the data is imbalanced, i.e., one class contains many more samples than the other. In such cases, using regular k-fold cross-validation could result in folds that contain only samples of the majority class, which would lead to a biased model. Stratified k-fold cross-validation ensures that each fold contains a representative sample of the overall dataset, leading to a more robust and generalizable model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d)\n",
    "\n",
    "the challenges of validating clustering results in the absence of ground truth labels:\n",
    "\n",
    "- Determining the Optimal Number of Clusters: Without ground truth labels, it can be difficult to determine the optimal number of clusters. Techniques like the elbow method or silhouette analysis can be used, but they may not always provide clear answers.\n",
    "\n",
    "- Interpreting the Clusters: Without ground truth, interpreting the meaning of the clusters can be subjective and depends heavily on domain knowledge. It can be challenging to determine whether the clusters correspond to meaningful groupings in the data.\n",
    "\n",
    "- Evaluating the Quality of the Clustering: Without ground truth labels, it’s hard to objectively evaluate the quality of the clustering. Metrics like cohesion (how closely related are items within the same cluster) and separation (how different are items from different clusters) can be used, but these are not always indicative of a “good” clustering from a practical perspective.\n",
    "\n",
    "Despite these challenges, there are several metrics that can be used to validate clustering results even in the absence of ground truth labels:\n",
    "\n",
    "- Silhouette Coefficient: This is a measure of how similar an object is to its own cluster compared to other clusters. The values lie in the range of -1 to 1. A high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "- Davies-Bouldin Index: This index signifies the average ‘similarity’ between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves. Zero is the lowest possible score. Values closer to zero indicate a better partition.\n",
    "\n",
    "- Calinski-Harabasz Index: Also known as the Variance Ratio Criterion, this index is the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters. Higher values indicate better clustering.\n",
    "\n",
    "domain-specific knowledge can be invaluable for validating clustering results. This knowledge can guide the interpretation of the clusters, help determine whether the clustering makes sense given the context, and provide a qualitative form of validation. For example, in customer segmentation, domain knowledge about customer behavior can help validate and interpret the clusters. In bioinformatics, knowledge about biological processes can be used to validate clusters of genes or proteins.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e)\n",
    "\n",
    "1. Elbow Method: The Elbow method is a heuristic method of interpretation and validation of consistency within cluster analysis designed to help find the appropriate number of clusters in a dataset. It uses the concept of WCSS (Within Cluster Sum of Squares). In this method, the number of clusters varies from 1 to n (total number of observations). For each number, the WCSS is calculated. A plot of WCSS against the number of clusters will show a sharp bend (or “elbow”), which indicates the optimal number of clusters. This bend signifies that adding another cluster doesn’t significantly improve the fit, i.e., the within-cluster variance doesn’t decrease significantly with each additional cluster.\n",
    "\n",
    "2. Gap Statistic: The Gap statistic compares the total intracluster variation for different values of k with their expected values under null reference distribution of the data. The optimal number of clusters is usually where the gap statistic reaches its maximum.\n",
    "\n",
    "3. Silhouette Analysis: Silhouette analysis measures how close each point in one cluster is to the points in the neighboring clusters. The silhouette score ranges from -1 to 1. A high silhouette score indicates that the point is well-matched to its own cluster and poorly matched to neighboring clusters. If most points have a high silhouette value, then the clustering configuration is appropriate. If many points have a low or negative silhouette value, then the clustering configuration may have too many or too few clusters. The optimal number of clusters is the one that maximizes the average silhouette over a range of possible values for k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f)\n",
    "\n",
    "A lower value of k (such as 2 or 3) will have high bias and low variance, leading to a risk of underfitting, while a higher value of k (such as 10 or more) will have low bias and high variance, leading to a risk of overfitting.\n",
    "\n",
    "A common choice for k is 5 or 10, as these values have been found empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance. However, the optimal choice of k will depend on the specific characteristics of your dataset and the learning algorithm you’re using. It’s often a good idea to experiment with different values of k to find the one that provides the best bias-variance tradeoff for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# g)\n",
    "\n",
    "Out-of-Bag (OOB) error estimation is a method used in ensemble learning techniques, such as bagging, to estimate the generalization error of a model\n",
    "\n",
    "- How it’s computed: In bagging, each individual model is trained on a different subset of the training data, chosen randomly with replacement (bootstrap samples). This means that, on average, each bootstrap sample uses about two-thirds of the observations. The remaining one-third of the observations not used to fit a given bootstrap sample are referred to as the Out-of-Bag (OOB) observations.\n",
    "\n",
    "    For each observation, we can make predictions using only the models where that observation was OOB. The OOB error is then calculated as the average error for each observation calculated using predictions from the models that did not have that observation in their bootstrap sample.\n",
    "\n",
    "- Why it’s useful: The OOB error estimate provides a way to estimate the test error without the need for cross-validation or a separate test set, which can be particularly useful when dealing with small datasets. This makes it a computationally efficient way to estimate the generalization error of the ensemble model.\n",
    "\n",
    "    Moreover, the OOB error estimate can also be used for model selection, such as selecting the optimal number of trees in a Random Forest, or tuning other hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h)\n",
    "\n",
    "PCA is an unsupervised machine learning algorithm that combines correlated dimensions into a single new variable. This new variable represents an axis or line in the dataset that describes the maximum amount of variation and becomes the first principal component (PC).\n",
    "\n",
    "In genomics, PCA can be used to recognize differences in genetics between populations. For example, it can be used to infer cryptic population structure from genome-wide data such as single nucleotide polymorphisms (SNPs). This is based on the fact that such population structure can confound SNP-phenotype associations, resulting in some SNPs spuriously being called as associated with the phenotype (false positives).\n",
    "\n",
    "Moreover, PCA can also be used to identify outlier individuals which may need to be removed prior to further analyses, such as genome-wide association studies (GWAS). This is particularly useful in the field of personalized medicine, where treatments are tailored based on one’s genetic makeup.\n",
    "\n",
    "One of the key advantages of PCA in genomics is its ability to handle large-scale genome-wide data efficiently. For instance, a tool like flashpca can perform PCA of 15,000 individuals up to 125 times faster than existing tools, with identical results. This makes PCA an essential tool in genomics as traditional approaches may not adequately scale with the increasing size of SNP datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i)\n",
    "\n",
    "SVMs can be effective for anomaly detection because they find the hyperplane that best separates the normal data points from the anomalies. Mainly, the one-class support vector machine is an unsupervised model for anomaly or outlier detection.\n",
    "\n",
    "Effective for High-Dimensional Data: SVMs perform well in high-dimensional spaces, making them suitable for datasets with many features, such as those commonly encountered in anomaly detection tasks.\n",
    "\n",
    "Robust to Overfitting: SVMs are less prone to overfitting, which is crucial in anomaly detection where the goal is to generalize well to unseen anomalies.\n",
    "\n",
    "Optimal Separation: SVMs aim to find the hyperplane that maximally separates the normal data points from the anomalies, making them effective in identifying outliers.\n",
    "\n",
    "One-Class SVM: The One-Class SVM variant is specifically designed for anomaly detection, learning to distinguish normal data points from outliers without the need for labeled anomalies.\n",
    "\n",
    "Kernel Trick: SVMs can use kernel functions to map data into a higher-dimensional space, allowing for non-linear separation of anomalies from normal data.\n",
    "\n",
    "Handling Imbalanced Data: Anomaly detection datasets are often highly imbalanced, with normal data points outnumbering anomalies. SVMs can handle this imbalance well.\n",
    "\n",
    "Interpretability: SVMs provide clear decision boundaries, which can help in interpreting why a particular data point is classified as an anomaly.\n",
    "\n",
    "To demonstrate how to use Support Vector Machines for anomaly detection, we can use a sample dataset. The process involves importing necessary libraries for data processing, visualization, model training, and evaluation. Then, we train a one-class SVM model and predict anomalies from the model. We can also change the default threshold for anomaly prediction and visualize the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>کی گفته مرد گریه نمیکنه!؟!؟ سیلم امشب سیل #اصفهان</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>عکسی که چند روز پیش گذاشته بودم این فیلم الانش...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تنهاییم شبیه تنهاییه ظهرای بچگیم شده وقتی که ه...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>خوبه تمام قسمت‌های گوشی رو محافظت می‌کنه</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>این خاک مال مردمان است نه حاکمان #ایران #مهسا_...</td>\n",
       "      <td>ANGRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4919</th>\n",
       "      <td>من از بو و ماندگاریش راضی بودم ، قیمتش هم‌ مناسبه</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>گاز نداریم آب نداریم برق نداریم نت نداریم پول ...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4921</th>\n",
       "      <td>یکی بهم گفت برنو چرا عاشق نمیشی گفتم ما پول عا...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4922</th>\n",
       "      <td>زیادی داریم به قضیه ی گاز میپردازیم فقط فراخوا...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>سلام. خیلی مواظبت کنید این ویروس کوفتی رو‌ نگی...</td>\n",
       "      <td>SAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4924 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0      1\n",
       "0     کی گفته مرد گریه نمیکنه!؟!؟ سیلم امشب سیل #اصفهان    SAD\n",
       "1     عکسی که چند روز پیش گذاشته بودم این فیلم الانش...  OTHER\n",
       "2     تنهاییم شبیه تنهاییه ظهرای بچگیم شده وقتی که ه...    SAD\n",
       "3              خوبه تمام قسمت‌های گوشی رو محافظت می‌کنه  HAPPY\n",
       "4     این خاک مال مردمان است نه حاکمان #ایران #مهسا_...  ANGRY\n",
       "...                                                 ...    ...\n",
       "4919  من از بو و ماندگاریش راضی بودم ، قیمتش هم‌ مناسبه  HAPPY\n",
       "4920  گاز نداریم آب نداریم برق نداریم نت نداریم پول ...    SAD\n",
       "4921  یکی بهم گفت برنو چرا عاشق نمیشی گفتم ما پول عا...    SAD\n",
       "4922  زیادی داریم به قضیه ی گاز میپردازیم فقط فراخوا...  OTHER\n",
       "4923  سلام. خیلی مواظبت کنید این ویروس کوفتی رو‌ نگی...    SAD\n",
       "\n",
       "[4924 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_f = pd.read_excel('train_data.xlsx' , header=None)\n",
    "d_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4924</td>\n",
       "      <td>4924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4924</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>کی گفته مرد گریه نمیکنه!؟!؟ سیلم امشب سیل #اصفهان</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0      1\n",
       "count                                                4924   4924\n",
       "unique                                               4924      5\n",
       "top     کی گفته مرد گریه نمیکنه!؟!؟ سیلم امشب سیل #اصفهان  HAPPY\n",
       "freq                                                    1   1462"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(d_f.isnull().sum())\n",
    "d_f.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = d_f[0]\n",
    "Y = d_f[1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(pd.concat([y_train, y_test]))\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "Y = le.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ANGRY': 0, 'FEAR': 1, 'HAPPY': 2, 'OTHER': 3, 'SAD': 4}\n"
     ]
    }
   ],
   "source": [
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '10' '100' ... '۹۸' '۹۹' 'ᴇxᴏᴇɴᴛʏ']\n",
      "(3939, 9067)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "# vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(x_train)\n",
    "\n",
    "X_train = vectorizer.transform(x_train)\n",
    "X_test = vectorizer.transform(x_test)\n",
    "\n",
    "X = vectorizer.transform(X)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(feature_names)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k=300)\n",
    "selector.fit(X_train, y_train)\n",
    "X_train = selector.transform(X_train)\n",
    "X_test = selector.transform(X_test)\n",
    "\n",
    "X = selector.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 macro : [0.45416389841668164, 0.4707145057642932, 0.46362207359993696, 0.468107705290217, 0.46427193881426926, 0.4712112913346626, 0.47126106307574134, 0.4649830080484906, 0.4593291166008461, 0.4563849182759555]\n",
      "mean f1 macro : 0.47\n",
      "accuracy : [0.4934010152284264, 0.5035532994923858, 0.49746192893401014, 0.5055837563451777, 0.5055837563451777, 0.4984771573604061, 0.5045685279187817, 0.49746192893401014, 0.4954314720812183, 0.49746192893401014]\n",
      "mean accuracy : 0.5\n"
     ]
    }
   ],
   "source": [
    "def ceiltoup(x):\n",
    "  return math.ceil(x * 100) / 100.0\n",
    "\n",
    "f1_macro = []\n",
    "accuracy = []\n",
    "for i in range(10):\n",
    "  forest_model = DecisionTreeClassifier()\n",
    "  forest_model.fit(X_train,y_train)\n",
    "  y_pred = forest_model.predict(X_test)\n",
    "  f1_macro.append(skm.f1_score(y_test, y_pred, average=\"macro\"))\n",
    "  accuracy.append(skm.accuracy_score(y_test, y_pred, normalize=True))\n",
    "print('f1 macro : ' + str(f1_macro) + '\\nmean f1 macro : ' + str(ceiltoup(np.mean(f1_macro))))\n",
    "print('accuracy : ' + str(accuracy) + '\\nmean accuracy : ' + str(ceiltoup(np.mean(accuracy))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 macro : [0.5128078932200846, 0.5195105484783635, 0.5250215532176058, 0.5101474296596248, 0.5128643632700078, 0.5088507111820842, 0.5110202512846426, 0.5084720321042921, 0.5167660774856431, 0.5137581933791211]\n",
      "mean f1 macro : 0.52\n",
      "accuracy : [0.5512690355329949, 0.5543147208121827, 0.5634517766497462, 0.5532994923857868, 0.550253807106599, 0.5482233502538071, 0.5532994923857868, 0.5451776649746193, 0.5532994923857868, 0.5492385786802031]\n",
      "mean accuracy : 0.56\n"
     ]
    }
   ],
   "source": [
    "f1_macro = []\n",
    "accuracy = []\n",
    "for i in range(10):\n",
    "  forest_model = RandomForestClassifier()\n",
    "  forest_model.fit(X_train,y_train)\n",
    "  y_pred = forest_model.predict(X_test)\n",
    "  f1_macro.append(skm.f1_score(y_test, y_pred, average=\"macro\"))\n",
    "  accuracy.append(skm.accuracy_score(y_test, y_pred, normalize=True))\n",
    "print('f1 macro : ' + str(f1_macro) + '\\nmean f1 macro : ' + str(ceiltoup(np.mean(f1_macro))))\n",
    "print('accuracy : ' + str(accuracy) + '\\nmean accuracy : ' + str(ceiltoup(np.mean(accuracy))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 macro : [0.5272349219328182, 0.5272349219328182, 0.5272349219328182, 0.5272349219328182, 0.5272349219328182, 0.5272349219328182, 0.5272349219328182, 0.5272349219328182, 0.5272349219328182, 0.5272349219328182]\n",
      "mean f1 macro : 0.53\n",
      "accuracy : [0.5634517766497462, 0.5634517766497462, 0.5634517766497462, 0.5634517766497462, 0.5634517766497462, 0.5634517766497462, 0.5634517766497462, 0.5634517766497462, 0.5634517766497462, 0.5634517766497462]\n",
      "mean accuracy : 0.57\n"
     ]
    }
   ],
   "source": [
    "f1_macro = []\n",
    "accuracy = []\n",
    "for i in range(10):\n",
    "  svc_model = SVC(gamma='auto')\n",
    "  svc_model.fit(X_train,y_train)\n",
    "  y_pred = svc_model.predict(X_test)\n",
    "  f1_macro.append(skm.f1_score(y_test, y_pred, average=\"macro\"))\n",
    "  accuracy.append(skm.accuracy_score(y_test, y_pred, normalize=True))\n",
    "print('f1 macro : ' + str(f1_macro) + '\\nmean f1 macro : ' + str(ceiltoup(np.mean(f1_macro))))\n",
    "print('accuracy : ' + str(accuracy) + '\\nmean accuracy : ' + str(ceiltoup(np.mean(accuracy))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# report\n",
    "\n",
    "#### Data Loading and Preprocessing:\n",
    "- train_data.xlsx is loaded into a DataFrame d_f without headers.\n",
    "- Null values are checked and summarized with d_f.isnull().sum().\n",
    "- Descriptive statistics are generated with d_f.describe().\n",
    "- The first two columns are separated into features X and target Y.\n",
    "#### Data Splitting:\n",
    "- The dataset is split into training and testing sets using train_test_split with a test size of 20% and a random state for reproducibility.\n",
    "#### Label Encoding:\n",
    "- A LabelEncoder is fitted to the concatenated y_train and y_test to encode target labels.\n",
    "- The mapping of original class names to encoded labels is printed.\n",
    "#### Text Vectorization:\n",
    "- A TfidfVectorizer is used to convert text data into TF-IDF feature vectors.\n",
    "- The vectorizer is fitted to the training data and used to transform both training and testing sets.\n",
    "- The feature names are printed, and the shape of the transformed training data is outputted.\n",
    "#### Feature Scaling:\n",
    "- StandardScaler is applied to the data without centering the mean since the data is sparse.\n",
    "#### Feature Selection:\n",
    "- SelectKBest with f_classif is used to select the top 300 features based on the ANOVA F-value.\n",
    "##### Model Training and Evaluation:\n",
    "- Three different models are trained and evaluated: DecisionTreeClassifier, RandomForestClassifier, and SVC with gamma='auto'.\n",
    "- For each model, the process is repeated 10 times, and the F1 macro score and accuracy are calculated for each iteration.\n",
    "- The results are printed along with the mean F1 macro score and mean accuracy, rounded up to two decimal places using the ceiltoup function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0\n",
      "0  OTHER\n",
      "1  HAPPY\n",
      "2  ANGRY\n",
      "3  HAPPY\n",
      "4  OTHER\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>صعب روزی، بوالعجب کاری، پریشان عالمی</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>بسیار نرم و لطیف بوده و کیفیت بالایی داره.</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اصلا رنگش با چیزی که تو عکس بود خیلی فرق داشت</td>\n",
       "      <td>ANGRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>خیلی زیبا و ب اندازه و با دقت طراحی شده</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>سبزی پلو با ماهی مال عید نوروزه، امشب سوشی میخ...</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>سرخط خبرهای ۶ عصر، پنجشنبه ۲۹ جدی ۱۴۰۱</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>بوی عالی  ماندگاری خوب خیلی خوشم امدش مرسی دیجی</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>گاز که نداریم اینترنت هم روش? #وطن</td>\n",
       "      <td>ANGRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>من چندتاشو برا مغازه گرفتم باطریاشون کلا خرابن!</td>\n",
       "      <td>HAPPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>خیلی بی کیفیت \\nحتی بچه ها هم باهاش بازی نمیکنن</td>\n",
       "      <td>ANGRY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>548 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0     0Y\n",
       "0                 صعب روزی، بوالعجب کاری، پریشان عالمی  OTHER\n",
       "1           بسیار نرم و لطیف بوده و کیفیت بالایی داره.  HAPPY\n",
       "2        اصلا رنگش با چیزی که تو عکس بود خیلی فرق داشت  ANGRY\n",
       "3              خیلی زیبا و ب اندازه و با دقت طراحی شده  HAPPY\n",
       "4    سبزی پلو با ماهی مال عید نوروزه، امشب سوشی میخ...  OTHER\n",
       "..                                                 ...    ...\n",
       "543            سرخط خبرهای ۶ عصر، پنجشنبه ۲۹ جدی ۱۴۰۱   OTHER\n",
       "544    بوی عالی  ماندگاری خوب خیلی خوشم امدش مرسی دیجی  HAPPY\n",
       "545                 گاز که نداریم اینترنت هم روش? #وطن  ANGRY\n",
       "546    من چندتاشو برا مغازه گرفتم باطریاشون کلا خرابن!  HAPPY\n",
       "547    خیلی بی کیفیت \\nحتی بچه ها هم باهاش بازی نمیکنن  ANGRY\n",
       "\n",
       "[548 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"3rdHW_test.csv\" , header=None)\n",
    "Test= test[0]\n",
    "\n",
    "Test = vectorizer.transform(Test)\n",
    "Test = scaler.transform(Test)\n",
    "Test = selector.transform(Test)\n",
    "\n",
    "model = SVC(gamma='auto')\n",
    "model.fit(X,Y)\n",
    "\n",
    "test_lable = model.predict(Test)\n",
    "test_lable = pd.DataFrame(test_lable)\n",
    "\n",
    "test_lable.replace(to_replace=[0, 1 , 2 , 3 , 4],\n",
    "           value= ['ANGRY', 'FEAR', 'HAPPY', 'OTHER', 'SAD'], \n",
    "           inplace=True)\n",
    "\n",
    "print(test_lable.head())\n",
    "answer =  test.join(test_lable[0] , rsuffix=\"Y\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
